{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004c087-db75-4259-827e-b90811dcde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e27bd0-ce44-4886-b026-3f322c2cadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import (AutoTokenizer,AutoModel)\n",
    "from intel_extension_for_transformers.transformers.modeling import AutoModelForCausalLM #Load model for CPU use only\n",
    "from huggingface_hub import interpreter_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dab080-f8e2-467b-b4b2-76d8271e53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loging to HuggingFace with API key\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d57a85-31f5-4a21-9146-37f5cf678b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c46061-f861-435d-9352-5d8705043dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for downloaded files\n",
    "\n",
    "cache_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e0c9e-9e10-4a3a-92fa-45d5365bd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1fd44-7cdc-4468-9134-ab4e36cc20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    '', device_map=\"cpu\", torch_dtype = torch.float16,\n",
    "    cache_dir = cache_dir, trust_remote_code=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08cfca6-d8bb-44a9-8d34-d6100c51dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Second method to download the model from HuggingFace\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "snapshot_download(repo_id=\"\",\n",
    "                cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35bd0b1-aade-4b83-a761-15eb8ab55eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c83132-9ed2-4689-93a9-fd98b825e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download certain files from HUggingFace\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "model_id = \"\"\n",
    "filenames = [ \n",
    "    \"model.safetensors\", \"tokenizer.json\", \"\"\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    downloaded_model_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=filename,\n",
    "        cache_dir = cache_dir\n",
    "    )\n",
    "\n",
    "    print(downloaded_model_path)\n",
    "\n",
    "print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8f9bc-7979-44a7-a893-22f628eb2a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff18a69-4540-4f2d-b1ea-cfd2c8c016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer for the specified model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ad2a5-da12-4f40-8b99-6d83c7b173d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff216bb-0efd-4cdd-85c5-b19dc9773ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model for sequence-to-sequence tasks using the specified pretrained model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, \n",
    "                                             low_cpu_mem_usage=True, device_map=\"cpu\", trust_remote_code=True, attn_implementation='eager')\n",
    "                                             #attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad98ab4-5d91-4d82-8e1a-6afb5a99a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfc77d-5bb1-44a5-ba88-309d824004b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8520596-4782-4649-892c-eb37b5c9bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420f9ab-2785-4057-82e2-cdd9c0ea24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text-to-text generation using a specified model and tokenizer\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",  # Specify the task as text-to-text generation\n",
    "    model=model,              # Use the previously initialized model\n",
    "    tokenizer=tokenizer,      # Use the previously initialized tokenizer\n",
    "    max_length=512,           # Set the maximum length for generated text to 512 tokens\n",
    "    temperature=0.1,          # Set the temperature parameter for controlling randomness (0 means deterministic)\n",
    "    top_p=0.95,               # Set the top_p parameter for controlling the nucleus sampling (higher values make the output more focused)\n",
    "    repetition_penalty=1.15   # Set the repetition_penalty to control the likelihood of repeated words or phrases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1d65f-3732-4cb6-8dbc-f8bf42d7fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM \n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe,\n",
    "    model_kwargs={\"temperature\": 0.1}, # Alternative way to set arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5ea35-12f3-4b22-959b-b6aa831d12f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70508704-3bcb-456b-b0a5-8387def885a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GGUF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956e169-f9c6-4159-9086-dcc257636654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad48972-4cd2-4b58-844a-76f6cce647b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=r\".gguf\",\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa691d-9c82-407d-b31a-47ddf131d21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176db204-3782-40da-ae9f-8430ef71b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820688a1-68cc-435f-90ba-0fe0a40ef65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9383f8-5513-485d-b1af-5518e0dbb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GPT4All(\n",
    "        model=r\"\",\n",
    "        verbose=True, max_tokens=512,    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cba52-1e8c-4890-aeb2-ac77e1e53b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
